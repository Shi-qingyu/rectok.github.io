<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <title>RecTok: Reconstruction Distillation along Rectified Flow</title>
        <script src="template.v2.js"></script>
        <script src="https://d3js.org/d3.v5.min.js"></script>
        <script src="https://d3js.org/d3-collection.v1.min.js"></script>
        <script src="https://rawgit.com/nstrayer/slid3r/master/dist/slid3r.js"></script>
        <script src="cross_fade.js"></script>
        <link rel="stylesheet" href="style.css">
        <script>
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      displayMath: [['\\[','\\]'], ['$$','$$']],
      packages: {'[+]': ['ams', 'textmacros']}  // enables \text
    },
    svg: { fontCache: 'global' }
  };
</script>
<script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.css" integrity="sha384-n8MVd4RsNIU0tAv4ct0nTaAbDJwPJzDEaqSD1odI+WdtXRGWt2kTvGFasHpSy3SV" crossorigin="anonymous">
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/katex.min.js" integrity="sha384-XjKyOOlGwcjNTAIQHIpgOno0Hl1YQqzUOEleOLALmuqehneUG+vnGctmUb0ZY0l8" crossorigin="anonymous"></script>
        <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.9/dist/contrib/auto-render.min.js" integrity="sha384-+VBxd3r6XgURycqtZ117nYw44OOcIax56Z4dCRWbxyPt0Koah1uHoK0o4+/RRE05" crossorigin="anonymous"
            onload="renderMathInElement(document.body);"></script>
    </head>
    <body>
        <div class="header-container">
            <div class="header-content">
              <h1>RecTok: Reconstruction Distillation along Rectified Flow</h1>
              <p>RecTok overcomes the limitations of latent space dimensionality, achieving state-of-the-art generation performance through a high-dimensional, semantically rich latent space.</p>
              <div class="button-container">
                <a href="https://arxiv.org/abs/2510.11690" class="button">Paper</a>
                <a href="https://github.com/bytetriper/RAE" class="button">Code</a>
                <a href="https://huggingface.co/collections/nyu-visionx/rae-68ecb57b8bfbf816c83cce15" class="button">ðŸ¤— Models</a>
              </div>
            </div>
        </div>
    <d-article>
        <div class="byline">
            <div class="byline-container">
                <div class="byline-column">
                    <h3>Authors</h3>
                    <p><a href="http://bytetriper.github.io/" class="author-link">Qingyu Shi</a></p>
                    <p><a href="https://wusize.github.io/" class="author-link">Size Wu</a></p>
                    <p><a href="https://noyii.github.io/" class="author-link">Jinbin Bai</a></p>
                    <p><a href="https://openreview.net/profile?id=%7EKaidong_Yu1" class="author-link">Kaidong Yu</a></p>
                    <p><a href="https://scholar.google.com/citations?user=YgL4rywAAAAJ&hl=en" class="author-link">Yujing Wang</a></p>
                    <p><a href="https://scholar.google.com/citations?user=T4gqdPkAAAAJ&hl=zh-CN" class="author-link">Yunhai Tong</a></p>
                    <p><a href="https://lxtgh.github.io/" class="author-link">Xiangtai Li</a></p>
                    <p><a href="https://scholar.google.com/citations?user=ahUibskAAAAJ&hl=zh-CN" class="author-link">Xuelong Li</a></p>
                </div>
                <div class="byline-column">
                    <h3>Affiliations</h3>
                      <p><a href="https://www.pku.edu.cn/" class="affiliation-link">Peking University</a></p>
                      <p><a href="https://www.ntu.edu.sg/" class="affiliation-link">Nanyang Technological University</a></p>
                      <p><a href="https://github.com/Tele-AI" class="affiliation-link">Tele AI</a></p>
                </div>
                <div class="byline-column">
                    <h3>Resources</h3>
                    <p><a href="https://arxiv.org/abs/2510.11690" class="affiliation-link">Paper</a></p>
                    <p><a href="https://github.com/Shi-qingyu/RecTok" class="affiliation-link">Code Repository</a></p>
                    <p><a href="https://huggingface.co/QingyuShi/RecTok" class="affiliation-link">Hugging Face Models</a></p>
                </div>
            </div>
        </div>
        <d-contents>
            <nav>
                <h4>Contents</h4>
                <div><a href="#overview">Overview</a></div>
                <div><a href="#motivation">Motivation</a></div>
                <div><a href="#insight">Insight</a></div>
                <div><a href="#rectok">RecTok</a></div>
                <div><a href="#experiment">Experiment</a></div>
                <div><a href="#conclusion">Conclusion</a></div>
            </nav>
        </d-contents>

<section id="overview">
    <h2>Overview</h2>
    <p>
    Visual tokenizers play a crucial role in diffusion models. 
    The dimensionality of latent space governs both reconstruction fidelity and the semantic expressiveness of the latent feature. 
    However, a fundamental trade-off is inherent between dimensionality and generation quality, constraining existing methods to low-dimensional latent spaces. 
    Although recent works have leveraged vision foundation models (VFMs) to enrich the semantics of visual tokenizers and accelerate convergence, high-dimensional tokenizers still underperform their low-dimensional counterparts.
    </p>

    <p>
    In this work, we propose RecTok, which overcomes the limitations of high-dimensional visual tokenizers through two key innovations: flow semantic distillation and reconstruction alignment distillation.
    Our key insight is to make the forward flow in flow matching semantically rich, which serves as the training space of diffusion transformers, rather than focusing on the latent space as in previous works.
    Specifically, our method distill the semantic information in VFMs into the forward flow trajectories in flow matching. And we further enhance the semantics by introducing a masked feature reconstruction loss.
    </p>

    <p>
        Combining the strengths of high-dimensional latent spaces and our proposed inoovations:
        <ul>
            <li><b>RecTok achieves state-of-the-art results on the gFID-50K ImageNet-1K 256x256.</b></li>
            <li><b>RecTok achieves an effective balance among reconstruction, generation quality, and semantic representation.</b></li>
            <li><b>We demonstrate that all three aspects mentioned above can be consistently improved by increasing the dimensionality of the latent space.</b></li>
        </ul>
    </p>

    <d-figure>
      <figure class="l-body">
        <img src="images/rectok/teaser.png", alt="RAE architecture overview">
        <figcaption style="margin-top:20px;"></figcaption>
    </d-figure>
</section>

<section id="motivation">
    <h2>Motivation</h2>

    <h3>The Dimension Dilemma of Visual Tokenizers.</h3> 
    <p>   
      Increasing the latent dimension presents a challenge, as it often compromises the model's generation ability. 
      So previous tokenizers such as SD-VAE and SD3-VAE are restricted to a low-dimensional space, which in turn limits both reconstruction fidelity and semantic expressiveness.
    </p>

    <d-figure>
        <figure class="l-body">
            <img src="images/rectok/dim_dilemma.png" alt="Reconstruction examples produced by RAEs">
            <figcaption></figcaption>
        </figure>
    </d-figure>

    <h3>Semantic Enhancement in the Latent Space.</h3>
    <p>
    To address this limitation, previous methods distill semantic information from vision foundation models into the latent space, 
    aiming to enrich the latent representation capacity and accelerate downstream generation training's convergence. 
    These works effectively increase the dimension of latent space from 16 to 32.
    </p>
  
    <d-figure>
        <figure class="l-body">
            <img src="images/rectok/recent_works2.png" alt="Recent works">
            <figcaption></figcaption>
        </figure>
    </d-figure>

  <div style="max-width: 1000px; margin: 0 auto;">
    <table style="width:100%; margin-top:25px; margin-left: -20px; border-collapse:separate; border-spacing:12px; border: none;">
      <tr>
        <!-- Left column: text -->
        <td style="vertical-align:top; width:50%; text-align:justify; font-size: 1em;">
          <p>
          However, their generation quality in high dimensions still lags behind their low-dimensional counterparts: continuous to expand the latent dimension still leads to the degradation of generation quality. 
          On the contrary, RecTok overcome this bottleneck, we increase the dimension of latent space to 128 with consistent gain in reconstruction, generation, and linear probing.
          </p>
        </td>
        <!-- Right column: table -->
        <td style="vertical-align:top; width:50%;">
          <d-figure>
              <figure class="l-body">
                  <img src="images/rectok/motivation.png" alt="Reconstruction examples produced by RAEs">
                  <figcaption></figcaption>
              </figure>
          </d-figure>
        </td>
      </tr>
    </table>
  </div>
</section>

<section id="insight">
  <h2>Insight</h2>
  <p>
  Unlike previous works that directly inject semantics to the un-noised latent space, 
  our core insight is the denoising network is trained on the forward flow instead of the latent space, as shown in figure A. 
  To understand the importance of semantic consistency along the flow, we first evaluate the discriminative capability on the flow. As shown in figure B, 
  the linear probing accuracy of the previous tokenizer drops remarkably as the latent is propagated along the forward flow, the representations that DiT receives during diffusion training.
  </p>

  <d-figure>
      <figure class="l-body">
          <img src="images/rectok/insight.png" alt="Reconstruction examples produced by RAEs">
          <figcaption></figcaption>
      </figure>
  </d-figure>


  <h3>Enhacing the Semantics along the Rectified Flow</h3>

  <div style="max-width: 1000px; margin: 0 auto;">
    <table style="width:100%; margin-top:25px; margin-left: -20px; border-collapse:separate; border-spacing:12px; border: none;">
      <tr>
        <!-- Left column: text -->
        <td style="vertical-align:top; width:50%; text-align:justify; font-size: 1em;">
          <p>
          We take a more training consistent perspective: since DiT is trained on the forward flow rather than on latent feature, 
          we enhance the semantics of all flow states through two key innovations: Flow Semantic Distillation, FSD for short and Reconstruction and Alignment Distillation, RAD for short.
          </p>
        </td>
        <!-- Right column: table -->
        <td style="vertical-align:top; width:50%;">
          <figure class="l-body">
              <img src="images/rectok/insight2.png" alt="Reconstruction examples produced by RAEs">
              <figcaption></figcaption>
          </figure>
        </td>
      </tr>
    </table>
  </div>
</section>

<section id="rectok" style="margin-top:24px;">
  <h2>RecTok</h2>

  <div class="finding-box", style="margin-top: -20px;">
    <ul>
      <li><strong>Flow Semantic Distillation (FSD).</strong> Make every point $x_t$ along the forward flow discriminative and semantically rich.</li>
      <li><strong>Reconstruction and Alignment Distillation (RAD).</strong> Further enhance the semantics through reconstruction target.</li>
    </ul>
  </div>

  <figure class="l-body">
      <img src="images/rectok/pipeline.png" alt="Pipeline of RecTok">
      <figcaption>Pipeline of RecTok.</figcaption>
  </figure>
  
  <p>
    We show our RecTok above. During the training of RecTok, we apply a random mask to the input image and encode the visible regions using the encoder to obtain $x_1$. We then sample a time step $t$ and use the forward flow to generate the corresponding $x_t$. 
    Subsequently, $x_t$ is fed into two decoders: the Semantic Decoder reconstructs the features of VFMs, 
    while the Pixel Decoder reconstructs the pixel space. 
    After training, both the Semantic Decoder and VFMs are discarded, ensuring the efficiency of RecTok during inference.
    In the following sections, we further detail the FSD and RAD.
  </p>

  <h3>Flow Semantic Distillation</h3>

  <figure class="l-body">
      <img src="images/rectok/fsd.png" alt="FSD">
      <figcaption>Illustration of Flow Semantic Distillation.</figcaption>
  </figure>

  <p>
    First we will introduce the Flow Semantic Distillation. 
    We distill the semantics of the VFMs into the forward flow trajectory.
    Fortunately, the forward flow from data $x_0$ to noise $\epsilon$ is independent of the velocity network $v_\theta(x,t)$, 
    allowing us to obtain $x_t=(1-t)x_0+t\epsilon, \ t\in [0, 1]$ easily through interpolation between the encoded $x_0 = E_{\theta}(I)$ and Gaussian noise $\epsilon$. 
    Each $x_t$ is then decoded by a lightweight semantic decoder $D_{\text{sem}}$ to obtain semantic features, 
    which are supervised by VFMs features.
  </p>

  <p>
    Considering the redundancy in high-dimensional latent spaces, we apply a dimension-dependent shift to the distribution of $t$, following RAE, and sample it as follows:
  </p>

  <p>
    $$ t = \frac{s t'}{1 + (s - 1)t'}, \quad t' \sim \mathcal{U}(0,1), \quad s = \sqrt{\frac{4096}{r^2 d}} $$
  </p>

  <p>
    where $r, d$ is the resolution and dimension of the latent feature, respectively.
  </p>

  <h3>Reconstruction and Alignment Distillation</h3>

  <figure class="l-body">
      <img src="images/rectok/rad.png" alt="RAD">
      <figcaption>Illustratoin of Reconstruction and Alignment Distillation.</figcaption>
  </figure>

  <p>
    To further enhance the semantic along the forward flow, inspired by masked image modeling methods, which obtain semantically rich features through pixel or feature reconstruction, we introduce a reconstructive target during FSD. 
    Specifically, we apply random masks to the input image and reconstruct the missing regions based on the visible noisy latent features.
    To ensure compatibility with the reconstruction task, we utilize a transformer-based semantic decoder $D_{sem}$.
  </p>
</section>

<section id="experiment" style="margin-top:40px;">
  <h2>Experiment</h2>

  <h3>Tokenizer Comparison on ImageNet-1K</h3>
  <div style="max-width: 1000px; margin: 0 auto;">
    <table style="width:100%; margin-top:25px; margin-left: -20px; border-collapse:separate; border-spacing:12px; border: none;">
      <tr>
        <!-- Left column: text -->
        <td style="vertical-align:top; width:50%; text-align:justify; font-size: 1em;">
          <p>
            We compare RecTok with other representative tokenizers in terms of parameter count, GFLOPs, reconstruction quality, and generation performance.
            RecTok achieves the best performance among ViT-based tokenizers.
          </p>
        </td>
        <!-- Right column: table -->
        <td style="vertical-align:top; width:50%;">
          <figure class="l-body">
              <img src="images/rectok/insight2.png" alt="Reconstruction examples produced by RAEs">
              <figcaption></figcaption>
          </figure>
        </td>
      </tr>
    </table>
  </div>

  <h3>Generation Performance</h3>
  <p>
    In this work, we propose and study RAE and DiT<sup>DH</sup>. In <a href="#sec-theory">Section&nbsp;2</a>, we showed that
    RAE combined with DiT already brings substantial benefits, even without the additional head.
    Here, we turn the question around: can DiT<sup>DH</sup> still provide improvements without the latent space of RAE?
  </p>

  <figure style="float:left; width:40%; margin:0.5rem 1rem 0.5rem 0;">
    <table class="display-table" style="width:100%; font-size:0.9em;">
      <tr>
        <th></th>
        <th>VAE</th>
        <th>DINOv2-B</th>
      </tr>
      <tr>
        <td style="text-align:left;">DiT-XL</td>
        <td>7.13</td>
        <td>4.28</td>
      </tr>
      <tr style="background:#f7f7f7;">
        <td style="text-align:left;">DiT<sup>DH</sup>-XL</td>
        <td>11.70</td>
        <td><strong>2.16</strong></td>
      </tr>
    </table>
    <figcaption style="font-size:0.85em; margin-top:4px;">
      <strong>gFID-50k on VAE.</strong> DiT<sup>DH</sup> yields worse FID than DiT, despite using extra compute for
      the wide head.
    </figcaption>
  </figure>

  <h3>The Dimension of Latent Space</h3>
  <p>
    DiT<sup>DH</sup> achieves strong performance when paired with the high-dimensional latent space of RAE.
    This raises a key question: is the structured representation of RAE essential, or would DiT<sup>DH</sup> work equally well
    on unstructured high-dimensional inputs such as <em>raw pixels</em>?
  </p>

  <figure style="float:right; width:40%; margin:0.5rem 0 0.5rem 1rem;">
    <table class="display-table" style="width:100%; font-size:0.9em;">
      <tr>
        <th></th>
        <th>Pixel</th>
        <th>DINOv2-B</th>
      </tr>
      <tr>
        <td style="text-align:left;">DiT-XL</td>
        <td>51.09</td>
        <td>4.28</td>
      </tr>
      <tr style="background:#f7f7f7;">
        <td style="text-align:left;">DiT<sup>DH</sup>-XL</td>
        <td>30.56</td>
        <td><strong>2.16</strong></td>
      </tr>
    </table>
    <figcaption style="font-size:0.85em; margin-top:4px;">
      <strong>Comparison on pixel diffusion (gFID-50k).</strong> Pixel diffusion performs far worse than diffusion on DINOv2-B
      latents.
    </figcaption>
  </figure>

  <h3>T</h3>
  <p>
    DiT<sup>DH</sup> achieves strong performance when paired with the high-dimensional latent space of RAE.
    This raises a key question: is the structured representation of RAE essential, or would DiT<sup>DH</sup> work equally well
    on unstructured high-dimensional inputs such as <em>raw pixels</em>?
  </p>

  <figure style="float:right; width:40%; margin:0.5rem 0 0.5rem 1rem;">
    <table class="display-table" style="width:100%; font-size:0.9em;">
      <tr>
        <th></th>
        <th>Pixel</th>
        <th>DINOv2-B</th>
      </tr>
      <tr>
        <td style="text-align:left;">DiT-XL</td>
        <td>51.09</td>
        <td>4.28</td>
      </tr>
      <tr style="background:#f7f7f7;">
        <td style="text-align:left;">DiT<sup>DH</sup>-XL</td>
        <td>30.56</td>
        <td><strong>2.16</strong></td>
      </tr>
    </table>
    <figcaption style="font-size:0.85em; margin-top:4px;">
      <strong>Comparison on pixel diffusion (gFID-50k).</strong> Pixel diffusion performs far worse than diffusion on DINOv2-B
      latents.
    </figcaption>
  </figure>
</section>

<section id="conclusion" style="margin-top:40px;">
  <h2>Conclusion</h2>

  <p>
    In this work, we challenge the belief that pretrained representation encoders are too high-dimensional and too semantic
    for reconstruction or generation. We show that a frozen representation encoder, paired with a lightweight trained decoder,
    forms an effective <strong>Representation Autoencoder (RAE)</strong>. On this latent space, we train Diffusion Transformers
    in a stable and efficient way with three added components: (1) match DiT width to the encoder token dimensionality,
    (2) apply a dimension-dependent shift to the noise schedule, and (3) add decoder noise augmentation so the decoder
    handles diffusion outputs. We also introduce <strong>DiT<sup>DH</sup></strong>, a shallow-but-wide diffusion transformer
    head that increases width without quadratic compute. Empirically, RAEs enable strong visual generation: on ImageNet, our
    RAE-based DiT<sup>DH</sup>-XL achieves an FID of <strong>1.51</strong> at 256Ã—256 (no guidance) and
    <strong>1.13 / 1.13</strong> at 256Ã—256 and 512Ã—512 (with guidance). We believe RAE latents serve as a strong candidate
    for training diffusion transformers efficiently and robustly in future generative modeling research.
  </p>
</section>

    </d-article>
        <d-appendix>
            <h3>BibTeX</h3>
            <p class="bibtex">
                @misc{zheng2025diffusiontransformersrepresentationautoencoders,<br>
                &nbsp;&nbsp;title={Diffusion Transformers with Representation Autoencoders},<br>
                &nbsp;&nbsp;author={Boyang Zheng and Nanye Ma and Shengbang Tong and Saining Xie},<br>
                &nbsp;&nbsp;year={2025},<br>
                &nbsp;&nbsp;eprint={2510.11690},<br>
                &nbsp;&nbsp;archivePrefix={arXiv},<br>
                &nbsp;&nbsp;primaryClass={cs.CV}<br>
                }
            </p>
            <d-footnote-list></d-footnote-list>
            <d-citation-list></d-citation-list>
        </d-appendix>

        <!-- bibliography will be inlined during Distill pipeline's pre-rendering -->
    <d-bibliography src="bibliography.bib"></d-bibliography>
    <script src="contents_bar.js"></script> <!-- for scroll/toc -->
    </body>
</html>